# AOC
# Algorithm-to-Outcome Concordance (AOC) Calculator

This repository provides a reproducible implementation of the **Algorithm-to-Outcome Concordance (AOC)** metric —  
a novel quantitative framework for assessing how well an AI model's predictive performance translates to real-world clinical outcomes.

> Paper: *A Proposed Framework for Quantifying AI-to-Clinical Translation: The Algorithm-to-Outcome Concordance (AOC) Metric*, 2025.

## What is AOC?
The **AOC metric** integrates three components:
- **AUC (Area Under ROC Curve)** → predictive discrimination  
- **Corr (Correlation)** between model predictions and observed clinical outcomes  
- **I² (Heterogeneity)** to account for inter-cohort variability  

The formula is:
AOC = (AUC × Corr) / (1 + I² / 100)

AOC ranges from 0 to 1, and its interpretation is:
| AOC | Interpretation |
|------|----------------|
| <0.4 | Poor translational fidelity |
| 0.4–0.7 | Moderate fidelity |
| >0.7 | High fidelity |

---

## Quick Start

### 1.Clone this repo
```bash
git clone https://github.com/PillowSoprano/my_first_project
cd AOC-Calculator
```

### 2.Install dependencies

```bash
pip install -r requirements.txt
```

### 3.Run the example
```bash
python demo_example.py
```
AUC=0.92, Corr=0.82, AOC=0.754 → High fidelity

### Example Dataset
example_data.csv contains a small synthetic dataset (10 patients) with:
| Column | Description |
|---------|--------------|
| patient_id | Sample ID |
| predicted_score | AI model prediction (0–1) |
| actual_outcome | Clinical response (0/1) |


### Optional Web App
You can launch an interactive Streamlit demo with:
streamlit run app.py

### Citation
If you use the AOC framework or calculator, please cite:
Yu X, Fu K. Quantifying AI-to-Clinical Translation via the Algorithm-to-Outcome Concordance (AOC) Metric, 2025.

### License
MIT License – open for academic and non-commercial use.
